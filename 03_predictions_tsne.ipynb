{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ternary\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "TRUNCATION_COLOR = \"#ffffbf\"\n",
    "ONLAP_COLOR = \"#2c7bb6\"\n",
    "HORIZ_COLOR = \"#d7191c\"\n",
    " \n",
    "\n",
    "truncCmap = LinearSegmentedColormap.from_list(\"mycmap\", [\"#ffffff\", TRUNCATION_COLOR])\n",
    "onlapCmap = LinearSegmentedColormap.from_list(\"mycmap\", [\"#ffffff\", ONLAP_COLOR])\n",
    "horizCmap = LinearSegmentedColormap.from_list(\"mycmap\", [\"#ffffff\", HORIZ_COLOR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r'F:\\Geology\\WSGS\\Projects\\jupyter\\20neighbors.csv', index_col=[0])\n",
    "# data_subset0 = data.drop(['class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_NEIGHBORS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "DATASET = pd.read_csv(\n",
    "    r\"F:/Geology/WSGS/Projects/jupyter/0\" + str(NO_OF_NEIGHBORS) + \"neighbors.csv\",\n",
    "    index_col=[0],\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DATASET.drop(\"class\", axis=1),\n",
    "    DATASET[\"class\"],\n",
    "    test_size=0.1,  # don't forget to change this\n",
    "    random_state=86,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453703703703704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEIGH = KNeighborsClassifier(\n",
    "    algorithm=\"ball_tree\",\n",
    "    leaf_size=10,\n",
    "    metric=\"manhattan\",\n",
    "    metric_params=None,\n",
    "    n_jobs=None,\n",
    "    n_neighbors=5,\n",
    "    p=2,\n",
    "    weights=\"distance\",\n",
    ")\n",
    "NEIGH.fit(X_train, y_train)\n",
    "NEIGH.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    \"Flattens lists\"\n",
    "    for i in container:\n",
    "        if isinstance(i, (list, tuple)):\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "        else:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL = [\"thickness\", \"thickness natural log\", \"thickness power\"]\n",
    "FEATURES = []\n",
    "for item in INITIAL:\n",
    "    FEATURES.append(item)\n",
    "    for i in range(1, NO_OF_NEIGHBORS + 1):\n",
    "        FEATURES.append(item + \" neighbor \" + str(i))\n",
    "FEATURES.append([\"x location\", \"y location\", \"class\"])\n",
    "FLAT_FEATURES = list(flatten(FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "THICKENED = FLAT_FEATURES[0 : NO_OF_NEIGHBORS + 1]\n",
    "THICKENED.append(\"class\")\n",
    "LOGGED = FLAT_FEATURES[NO_OF_NEIGHBORS + 1 : 2 * NO_OF_NEIGHBORS + 2]\n",
    "LOGGED.append(\"class\")\n",
    "POWERED = FLAT_FEATURES[2 * NO_OF_NEIGHBORS + 2 : 3 * NO_OF_NEIGHBORS + 3]\n",
    "POWERED.append(\"class\")\n",
    "LOCATION = [\"x location\", \"y location\", \"class\"]\n",
    "OG_THICKNESS = [\"thickness\", \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without thickness accuracy is 0.956\n",
      "Without natural log. Accuracy is 0.81\n",
      "Without power accuracy is 0.95\n",
      "Without location accuracy is 0.94\n",
      "Done with well thickness. Accuracy is 0.95\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DATASET.drop(THICKENED, axis=1),\n",
    "    DATASET[\"class\"],\n",
    "    test_size=0.1,  # don't forget to change this\n",
    "    random_state=86,\n",
    ")\n",
    "NEIGH.fit(X_train, y_train)\n",
    "THICKNESS_REMOVED = NEIGH.score(X_test, y_test)\n",
    "print(f\"Without thickness accuracy is {THICKNESS_REMOVED:.3f}\")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DATASET.drop(LOGGED, axis=1),\n",
    "    DATASET[\"class\"],\n",
    "    test_size=0.1,  # don't forget to change this\n",
    "    random_state=86,\n",
    ")\n",
    "NEIGH.fit(X_train, y_train)\n",
    "LN_REMOVED = NEIGH.score(X_test, y_test)\n",
    "print(f\"Without natural log. Accuracy is {LN_REMOVED:.2f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DATASET.drop(POWERED, axis=1),\n",
    "    DATASET[\"class\"],\n",
    "    test_size=0.1,  # don't forget to change this\n",
    "    random_state=86,\n",
    ")\n",
    "NEIGH.fit(X_train, y_train)\n",
    "POWER_REMOVED = NEIGH.score(X_test, y_test)\n",
    "print(f\"Without power accuracy is {POWER_REMOVED:.2f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DATASET.drop(LOCATION, axis=1),\n",
    "    DATASET[\"class\"],\n",
    "    test_size=0.1,  # don't forget to change this\n",
    "    random_state=86,\n",
    ")\n",
    "NEIGH.fit(X_train, y_train)\n",
    "LOCATION_REMOVED = NEIGH.score(X_test, y_test)\n",
    "print(f\"Without location accuracy is {LOCATION_REMOVED:.2f}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DATASET.drop(OG_THICKNESS, axis=1),\n",
    "    DATASET[\"class\"],\n",
    "    test_size=0.1,  # don't forget to change this\n",
    "    random_state=86,\n",
    ")\n",
    "NEIGH.fit(X_train, y_train)\n",
    "OG_T_REMOVED = NEIGH.score(X_test, y_test)\n",
    "print(f\"Done with well thickness. Accuracy is {OG_T_REMOVED:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='ball_tree', leaf_size=10, metric='manhattan',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    DATASET.drop(\"class\", axis=1),\n",
    "    DATASET[\"class\"],\n",
    "    test_size=0.1,  # don't forget to change this\n",
    "    random_state=86,\n",
    ")\n",
    "NEIGH = KNeighborsClassifier(\n",
    "    algorithm=\"ball_tree\",\n",
    "    leaf_size=10,\n",
    "    metric=\"manhattan\",\n",
    "    metric_params=None,\n",
    "    n_jobs=None,\n",
    "    n_neighbors=5,\n",
    "    p=2,\n",
    "    weights=\"distance\",\n",
    ")\n",
    "NEIGH.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPS_API = pd.read_csv(\n",
    "    r\"F:\\Geology\\WSGS\\Projects\\Unconformity or onlap\\Python\\FTUNION.csv\"\n",
    ").fillna(\n",
    "    0\n",
    ")  # this file is available in the unconformity or onlap folder in the repo\n",
    "ITERABLE = [\"Kfh\", \"Kl\", \"Tfu\"]\n",
    "TOPCOMBOS = list(zip(ITERABLE, ITERABLE[1:]))\n",
    "\n",
    "# TOPCOMBOS.append((\"Kfh\", \"Kl\"))\n",
    "# TOPCOMBOS.append((\"Kl\", \"Tfu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on the subsurface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kfh', 'Kl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrp4932\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kl', 'Tfu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrp4932\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n"
     ]
    }
   ],
   "source": [
    "# run this for all combinations of 2 tops and KNN\n",
    "RESULTS = []\n",
    "NORM_ALL = []\n",
    "PROBS_ALL = []\n",
    "FULL_PROBS = []\n",
    "\n",
    "for j in enumerate(TOPCOMBOS):\n",
    "    print(TOPCOMBOS[j[0]])\n",
    "    TOPS_API = pd.read_csv(\n",
    "        r\"F:\\Geology\\WSGS\\Projects\\Unconformity or onlap\\Python\\FTUNION.csv\"\n",
    "    ).fillna(\n",
    "        0\n",
    "    )  # this file is available in the unconformity or onlap folder in the repo\n",
    "    fmtops = list(TOPCOMBOS[j[0]])\n",
    "    fmtops.extend([\"x\", \"y\"])\n",
    "    tops = TOPS_API[fmtops]\n",
    "\n",
    "    # calculate thicknesses and neighbors for the two tops\n",
    "    hood = squareform(pdist(tops.iloc[:, -2:]))\n",
    "    neighbors = []\n",
    "    for i in enumerate(hood.argsort()[0:, 1 : NO_OF_NEIGHBORS + 1]):\n",
    "        selected = (\n",
    "            tops.iloc[hood.argsort()[i[0], 1 : NO_OF_NEIGHBORS + 1], 0:-2]\n",
    "            .stack()\n",
    "            .to_frame()\n",
    "            .T\n",
    "        )\n",
    "        selected.columns = selected.columns.droplevel()\n",
    "        neighbors.append(selected)\n",
    "    frame = pd.concat(neighbors, sort=False)\n",
    "    frame.index = range(len(frame))\n",
    "    neighborhood = pd.concat([tops.iloc[:, :-2], frame], axis=1)\n",
    "    thicknesses = neighborhood.diff(axis=1) * -1\n",
    "    thicknesses[thicknesses < 0] = 0\n",
    "    thicknesses.drop(columns=tops.columns[0], inplace=True)\n",
    "    thicknesses[thicknesses < 0] = 0\n",
    "    thicknesses[thicknesses > 3000] = 0\n",
    "    locations = tops[[\"x\", \"y\"]]\n",
    "    real_world_log = thicknesses.apply(\n",
    "        np.log\n",
    "    )  # take the log of thicknesses for feature engineering\n",
    "    real_world_pow = thicknesses.apply(\n",
    "        lambda x: x ** 10\n",
    "    )  # calculates the power values of thickness for another feature\n",
    "    rw = (\n",
    "        pd.concat(\n",
    "            [thicknesses, real_world_log, real_world_pow, locations],\n",
    "            axis=1,\n",
    "            join_axes=[thicknesses.index],\n",
    "        )\n",
    "        .dropna()\n",
    "        .replace(-np.inf, 0)\n",
    "    )\n",
    "    normalized_rw = (rw - rw.min()) / (rw.max() - rw.min()).replace(\n",
    "        0, 0.00001\n",
    "    )  # normalize the data from 0 to 1\n",
    "    real_data = normalized_rw.values\n",
    "\n",
    "    well_preds = NEIGH.predict(real_data)  # knn predictions\n",
    "    well_prob = NEIGH.predict_proba(real_data)  # knn predictions\n",
    "    FULL_PROBS.append(well_prob)\n",
    "    probs = []\n",
    "    for i in range(len(well_prob)):\n",
    "        probs.append(well_prob[i].max())\n",
    "    PROBS_ALL.append(probs)\n",
    "    RESULTS.append(well_preds)\n",
    "    NORM_ALL.append(normalized_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "NORMALIZED_KL = NORM_ALL[0]\n",
    "NORMALIZED_TFU = NORM_ALL[1]\n",
    "\n",
    "NORMALIZED_KL.columns = DATASET.columns[0:-1].values\n",
    "NORMALIZED_KL[\"class\"] = RESULTS[0]\n",
    "NORMALIZED_KL[\"prob\"] = PROBS_ALL[0]\n",
    "\n",
    "NORMALIZED_TFU.columns = DATASET.columns[0:-1].values\n",
    "NORMALIZED_TFU[\"class\"] = RESULTS[1]\n",
    "NORMALIZED_TFU[\"prob\"] = PROBS_ALL[1]\n",
    "\n",
    "\n",
    "NORMALIZED_KL[\"Formation\"] = \"Kl\"  # this is lance\n",
    "NORMALIZED_TFU[\"Formation\"] = \"Tfu\"  # this is ft union\n",
    "\n",
    "DF_COMBINED = NORMALIZED_KL\n",
    "DF_COMBINED1 = DF_COMBINED.append(NORMALIZED_TFU, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's project the predictions down to 2D with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 151 nearest neighbors...\n",
      "[t-SNE] Indexed 1776 samples in 0.028s...\n",
      "[t-SNE] Computed neighbors for 1776 samples in 0.357s...\n"
     ]
    }
   ],
   "source": [
    "DF_SUBSET1 = DF_COMBINED1.drop([\"class\", \"Formation\", \"prob\"], axis=1)\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    verbose=0.2,\n",
    "    perplexity=50,\n",
    "    n_iter=1500,\n",
    "    learning_rate=500,\n",
    "    random_state=20,\n",
    ")  # per=250, iter = 500, lr=50\n",
    "TSNE_RESULTS = tsne.fit_transform(DF_SUBSET1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBABILITIES = np.vstack(FULL_PROBS)\n",
    "\n",
    "DF_COMBINED1[\"trunc_prob\"] = PROBABILITIES[:, 0]\n",
    "DF_COMBINED1[\"onlap_prob\"] = PROBABILITIES[:, 1]\n",
    "DF_COMBINED1[\"horiz_prob\"] = PROBABILITIES[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "DF_COMBINED1[\"tsne-2d-one\"] = TSNE_RESULTS[:, 0]\n",
    "DF_COMBINED1[\"tsne-2d-two\"] = TSNE_RESULTS[:, 1]\n",
    "# DF_COMBINED1['tsne-2d-three'] = TSNE_RESULTS[:,2]\n",
    "color_pals = [\"#ffffbf\", \"#2c7bb6\", \"#d7191c\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "# 0 is truncation, 1 is onlap, 2 is horizontal\n",
    "sns.scatterplot(\n",
    "    x=DF_COMBINED1[\"tsne-2d-one\"],\n",
    "    y=DF_COMBINED1[\"tsne-2d-two\"],\n",
    "    hue=DF_COMBINED1[\"trunc_prob\"],\n",
    "    style=DF_COMBINED1[\"Formation\"],\n",
    "    palette=truncCmap,\n",
    "    data=DF_COMBINED1,\n",
    "    legend=None,\n",
    "    alpha=1,\n",
    "    edgecolor=\"none\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "plt.xlim(-45, 45)\n",
    "plt.ylim(-45, 45)\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "# plt.savefig('tsne_trunc.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for onlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=DF_COMBINED1[\"tsne-2d-one\"],\n",
    "    y=DF_COMBINED1[\"tsne-2d-two\"],\n",
    "    hue=DF_COMBINED1[\"onlap_prob\"],\n",
    "    style=DF_COMBINED1[\"Formation\"],\n",
    "    palette=onlapCmap,\n",
    "    data=DF_COMBINED1,\n",
    "    legend=None,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"none\",\n",
    ")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.xlim(-45, 45)\n",
    "plt.ylim(-45, 45)\n",
    "# plt.savefig('tsne_onlap.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly horizontally stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "HORIZONTALS = DF_COMBINED1[(DF_COMBINED1.horiz_prob > 0.0)]\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=HORIZONTALS[\"tsne-2d-one\"],\n",
    "    y=HORIZONTALS[\"tsne-2d-two\"],\n",
    "    hue=HORIZONTALS[\"horiz_prob\"],\n",
    "    style=HORIZONTALS[\"Formation\"],\n",
    "    palette=horizCmap,\n",
    "    data=HORIZONTALS,\n",
    "    legend=None,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"none\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.xlim(-45, 45)\n",
    "plt.ylim(-45, 45)\n",
    "# plt.savefig('tsne_horiz.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to add in locations to the well predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_locs = TOPS_API.x.append(TOPS_API.x, ignore_index=True)\n",
    "y_locs = TOPS_API.y.append(TOPS_API.y, ignore_index=True)\n",
    "API = TOPS_API.API.append(TOPS_API.API, ignore_index=True)\n",
    "\n",
    "DF_COMBINED1[\"x_locs\"] = x_locs\n",
    "DF_COMBINED1[\"y_locs\"] = y_locs\n",
    "DF_COMBINED1[\"api\"] = API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split them out by formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTUNION = DF_COMBINED1[DF_COMBINED1[\"Formation\"] == \"Tfu\"]\n",
    "LANCER = DF_COMBINED1[DF_COMBINED1[\"Formation\"] == \"Kl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to save the predictions as a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(FTUNION.x_locs, FTUNION.y_locs)]\n",
    "crs = {\"init\": \"epsg:3732\"}\n",
    "geo_df = GeoDataFrame(FTUNION, crs={\"init\": \"epsg:4326\"}, geometry=geometry)\n",
    "geo_df.to_file(\n",
    "    driver=\"ESRI Shapefile\",\n",
    "    filename=r\"FTUNION_KNN_predictions_prob.shp\",\n",
    " )\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(LANCER.x_locs, LANCER.y_locs)]\n",
    "crs = {\"init\": \"epsg:3732\"}\n",
    "geo_df = GeoDataFrame(LANCER, crs={\"init\": \"epsg:4326\"}, geometry=geometry)\n",
    "geo_df.to_file(\n",
    "    driver=\"ESRI Shapefile\",\n",
    "    filename=r\"lance_KNN_predictions_prob.shp\",\n",
    " )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we want to make a ternary diagram of the prediction PROBABILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, tax = ternary.figure(scale=1)\n",
    "fig.set_size_inches(3, 3)\n",
    "\n",
    "tax.scatter(\n",
    "    LANCER[[\"trunc_prob\", \"onlap_prob\", \"horiz_prob\"]].values,\n",
    "    marker=\"o\",\n",
    "    alpha=0.6,\n",
    "    c=\"black\",\n",
    "    s=10,\n",
    ")\n",
    "tax.scatter(\n",
    "    FTUNION[[\"trunc_prob\", \"onlap_prob\", \"horiz_prob\"]].values,\n",
    "    marker=\"x\",\n",
    "    alpha=0.6,\n",
    "    c=\"black\",\n",
    "    s=10,\n",
    ")\n",
    "tax.left_axis_label(\"Truncation Probability\", fontsize=12, offset=0.08)\n",
    "tax.right_axis_label(\"Onlap Probability\", fontsize=12, offset=0.08)\n",
    "tax.bottom_axis_label(\"Horizontal Probability\", fontsize=12, offset=-0.08)\n",
    "tax.get_axes().axis(\"off\")\n",
    "\n",
    "tax.boundary(linewidth=1)\n",
    "tax.gridlines(multiple=0.20, color=\"gray\")\n",
    "tax.ticks(axis=\"lbr\", linewidth=1, multiple=0.20)\n",
    "tax.get_axes().axis(\"off\")\n",
    "# plt.savefig('ternary.pdf')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
