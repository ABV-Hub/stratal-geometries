{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING with 400\n",
      "Getting the features\n",
      "normalizing the truncation\n",
      "normalizing the onlap\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-055348e2bdb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1099\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#previously 399\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_layers\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             ] = 0\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[0mhood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquareform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrolling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mneighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[1;34m(self, key, is_setter)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Too many indexers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2175\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2176\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2177\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2012\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2015\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2016\u001b[0m             \u001b[1;31m# a tuple should already have been caught by this point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2086\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2087\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2088\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2090\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt\n",
    "\n",
    "names = [\n",
    "    \"one\",\n",
    "    \"two\",\n",
    "    \"three\"\n",
    "]  # this creates dummy names for the formations\n",
    "number_of_layers = (\n",
    "    2\n",
    ")  # this is the number of tops you want in your training data\n",
    "smallest = -5\n",
    "largest = 12\n",
    "step = 0.2\n",
    "for i in range(400,1000,100):\n",
    "\n",
    "    no_of_neighbors = i\n",
    "\n",
    "    np.random.seed(19)\n",
    "    df = pd.DataFrame()\n",
    "    locations = pd.DataFrame()\n",
    "    elevation_random = sorted(\n",
    "        np.random.uniform(smallest, largest, number_of_layers - 1)\n",
    "    )\n",
    "\n",
    "    print(f\"STARTING with {no_of_neighbors}\")\n",
    "    for j in np.arange(smallest, largest, step):\n",
    "        rolling = pd.DataFrame()\n",
    "        for i in range(len(names[0 : number_of_layers - 1])):\n",
    "            basement = 0.001 + (10 / j) * np.sin(\n",
    "                1 - np.arange(0, 40, 0.1) / (j * 10) + 0.001\n",
    "            )\n",
    "            elevation = np.full(400, j)\n",
    "            topbasement = np.where(basement > elevation, elevation, basement)\n",
    "            rolling[\"zero\"] = topbasement\n",
    "            layer_elevation = (\n",
    "                0.001\n",
    "                + (10 / j) * np.sin(1 - np.arange(0, 40, 0.1) / (j * 10) + 0.001)\n",
    "                + elevation_random[i]\n",
    "            )\n",
    "            layer_elevation = np.where(\n",
    "                layer_elevation > elevation, elevation, layer_elevation\n",
    "            )\n",
    "            rolling[names[i]] = layer_elevation\n",
    "        x = np.arange(0, 40, 0.1)\n",
    "        y = np.random.randint(0, 10, len(x))\n",
    "        if j % 0.2 > 0.1:\n",
    "            rolling[\"ex\"] = x * np.cos(-j / 2) - y * np.sin(-j / 2)\n",
    "            rolling[\"ey\"] = y * np.cos(-j / 2) - x * np.sin(-j / 2)\n",
    "        else:\n",
    "            rolling[\"ex\"] = x * np.cos(j / 2) - y * np.sin(j / 2)\n",
    "            rolling[\"ey\"] = y * np.cos(j / 2) - x * np.sin(j / 2)\n",
    "        for k in range(100):\n",
    "            rolling.iloc[\n",
    "                np.random.randint(0, 399),\n",
    "                np.random.randint(0, number_of_layers - 1),\n",
    "            ] = 0\n",
    "        hood = squareform(pdist(rolling.iloc[:, -2:]))\n",
    "        neighbors = []\n",
    "        for i in enumerate(hood.argsort()[0:, 1 : no_of_neighbors + 1]):\n",
    "            selected = (\n",
    "                rolling.iloc[hood.argsort()[i[0], 1 : no_of_neighbors + 1], 0:-2]\n",
    "                .stack()\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            selected.columns = selected.columns.droplevel()\n",
    "            neighbors.append(selected)\n",
    "        frame = pd.concat(neighbors, sort=False)\n",
    "        frame.index = range(len(frame))\n",
    "        neighborhood = pd.concat([rolling.iloc[:, :-2], frame], axis=1)\n",
    "        thicknesses = neighborhood.diff(axis=1)\n",
    "        thicknesses[thicknesses < 0] = 0\n",
    "        thicknesses.drop(columns=\"zero\", inplace=True)\n",
    "        locations = pd.concat((locations, rolling.iloc[:, -2:]))\n",
    "        df = pd.concat((df, thicknesses))\n",
    "    logged = df.apply(\n",
    "        np.log\n",
    "    )  # take the log of thicknesses for feature engineering\n",
    "    powered = df.apply(\n",
    "        lambda x: x ** 10\n",
    "    )  # calculates the power values of thickness for another feature\n",
    "    at = (\n",
    "        pd.concat([df, logged, powered, locations], axis=1, join_axes=[df.index])\n",
    "        .dropna()\n",
    "        .replace(-np.inf, 0)\n",
    "    )\n",
    "    print('Getting the features')\n",
    "    initial = ['thickness', 'thickness natural log', 'thickness power']\n",
    "    features = []\n",
    "    for item in initial:\n",
    "        features.append(item)\n",
    "        for i in range(1,no_of_neighbors+1):\n",
    "            features.append(item+' neighbor '+str(i))\n",
    "    features.append(['x location', 'y location', 'class'])\n",
    "    flat_features = flatten(features)\n",
    "    print('normalizing the truncation')\n",
    "    # NORMALIZING THE DATA\n",
    "    # normalize the data from 0 to 1\n",
    "    normalized_dfa = (at - at.min()) / (at.max() - at.min()).replace(0, 0.00001)\n",
    "    normalized_locations = (locations - locations.min()) / (\n",
    "        locations.max() - locations.min()\n",
    "    )\n",
    "    x = normalized_locations.ex.values\n",
    "    y = normalized_locations.ey.values\n",
    "    normalized_dfa[\"ex\"] = x\n",
    "    normalized_dfa[\"ey\"] = y\n",
    "\n",
    "    np.random.seed(19)\n",
    "\n",
    "    df_onlap = pd.DataFrame()\n",
    "    locations = pd.DataFrame()\n",
    "    for j in np.arange(smallest, largest, step):\n",
    "        rolling = pd.DataFrame()\n",
    "        for i in range(len(names[0 : number_of_layers - 1])):\n",
    "            basement = 0.001 + (10 / j) * np.sin(\n",
    "                1 - np.arange(0, 40, 0.1) / (j * 10) + 0.001\n",
    "            )\n",
    "            elevation = np.full(400, j)\n",
    "            topbasement = np.where(basement > elevation, elevation, basement)\n",
    "            rolling[\"zero\"] = topbasement\n",
    "            strat_elevation = np.full(400, elevation_random[i])\n",
    "            onlap = np.where(strat_elevation > basement, strat_elevation, basement)\n",
    "            layer_elevation = np.where(onlap > elevation, elevation, onlap)\n",
    "            rolling[names[i]] = layer_elevation\n",
    "        x = np.arange(0, 40, 0.1)\n",
    "        y = np.random.randint(0, 10, len(x))\n",
    "        if j % 0.2 > 0.1:\n",
    "            rolling[\"ex\"] = x * np.cos(-j / 2) - y * np.sin(-j / 2)\n",
    "            rolling[\"ey\"] = y * np.cos(-j / 2) - x * np.sin(-j / 2)\n",
    "        else:\n",
    "            rolling[\"ex\"] = x * np.cos(j / 2) - y * np.sin(j / 2)\n",
    "            rolling[\"ey\"] = y * np.cos(j / 2) - x * np.sin(j / 2)\n",
    "        for k in range(100):\n",
    "            rolling.iloc[\n",
    "                np.random.randint(0, 399),\n",
    "                np.random.randint(0, number_of_layers - 1),\n",
    "            ] = 0\n",
    "        hood = squareform(pdist(rolling.iloc[:, -2:]))\n",
    "        neighbors = []\n",
    "        for i in enumerate(hood.argsort()[0:, 1 : no_of_neighbors + 1]):\n",
    "            selected = (\n",
    "                rolling.iloc[hood.argsort()[i[0], 1 : no_of_neighbors + 1], 0:-2]\n",
    "                .stack()\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            selected.columns = selected.columns.droplevel()\n",
    "            neighbors.append(selected)\n",
    "        frame = pd.concat(neighbors, sort=False)\n",
    "        frame.index = range(len(frame))\n",
    "        neighborhood = pd.concat([rolling.iloc[:, :-2], frame], axis=1)\n",
    "        thicknesses = neighborhood.diff(axis=1)\n",
    "        thicknesses[thicknesses < 0] = 0\n",
    "        thicknesses.drop(columns=\"zero\", inplace=True)\n",
    "        locations = pd.concat((locations, rolling.iloc[:, -2:]))\n",
    "        df_onlap = pd.concat((df_onlap, thicknesses))\n",
    "    onlaplogged = df_onlap.apply(\n",
    "        np.log\n",
    "    )  # take the log of thicknesses for feature engineering\n",
    "    onlappowered = df_onlap.apply(\n",
    "        lambda x: x ** 10\n",
    "    )  # calculates the power values of thickness for another feature\n",
    "    ot = (\n",
    "        pd.concat(\n",
    "            [df_onlap, onlaplogged, onlappowered, locations],\n",
    "            axis=1,\n",
    "            join_axes=[df_onlap.index],\n",
    "        )\n",
    "        .dropna()\n",
    "        .replace(-np.inf, 0)\n",
    "    )\n",
    "    print('normalizing the onlap')\n",
    "    # NORMALIZING THE DATA\n",
    "    # normalize the data from 0 to 1\n",
    "    normalized_dfo = (ot - ot.min()) / (ot.max() - ot.min()).replace(0, 0.00001)\n",
    "    normalized_locations = (locations - locations.min()) / (\n",
    "        locations.max() - locations.min()\n",
    "    )\n",
    "    x = normalized_locations.ex.values\n",
    "    y = normalized_locations.ey.values\n",
    "    normalized_dfo[\"ex\"] = x\n",
    "    normalized_dfo[\"ey\"] = y\n",
    "\n",
    "    np.random.seed(19)\n",
    "\n",
    "    df_horizontal = pd.DataFrame()\n",
    "    locations = pd.DataFrame()\n",
    "    for j in np.arange(smallest, largest, step):\n",
    "        rolling = pd.DataFrame()\n",
    "        for i in range(len(names[0 : number_of_layers - 1])):\n",
    "            basement = np.full(400, 0) - np.random.rand(400) / 100\n",
    "            elevation = np.full(400, j)\n",
    "            topbasement = np.where(basement > elevation, elevation, basement)\n",
    "            rolling[\"zero\"] = topbasement\n",
    "            strat_elevation = np.full(400, elevation_random[i])\n",
    "            layer_elevation = np.where(\n",
    "                strat_elevation > elevation, elevation, strat_elevation\n",
    "            )\n",
    "            rolling[names[i]] = layer_elevation\n",
    "        x = np.arange(0, 40, 0.1)\n",
    "        y = np.random.randint(0, 10, len(x))\n",
    "        if j % 0.2 > 0.1:\n",
    "            rolling[\"ex\"] = x * np.cos(-j / 2) - y * np.sin(-j / 2)\n",
    "            rolling[\"ey\"] = y * np.cos(-j / 2) - x * np.sin(-j / 2)\n",
    "        else:\n",
    "            rolling[\"ex\"] = x * np.cos(j / 2) - y * np.sin(j / 2)\n",
    "            rolling[\"ey\"] = y * np.cos(j / 2) - x * np.sin(j / 2)\n",
    "        for k in range(100):\n",
    "            rolling.iloc[\n",
    "                np.random.randint(0, 399), \n",
    "                np.random.randint(0, number_of_layers - 1),\n",
    "            ] = 0\n",
    "        hood = squareform(pdist(rolling.iloc[:, -2:]))\n",
    "        neighbors = []\n",
    "        for i in enumerate(hood.argsort()[0:, 1 : no_of_neighbors + 1]):\n",
    "            selected = (\n",
    "                rolling.iloc[hood.argsort()[i[0], 1 : no_of_neighbors + 1], 0:-2]\n",
    "                .stack()\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            selected.columns = selected.columns.droplevel()\n",
    "            neighbors.append(selected)\n",
    "        frame = pd.concat(neighbors, sort=False)\n",
    "        frame.index = range(len(frame))\n",
    "        neighborhood = pd.concat([rolling.iloc[:, :-2], frame], axis=1)\n",
    "        thicknesses = neighborhood.diff(axis=1)\n",
    "        thicknesses[thicknesses < 0] = 0\n",
    "        thicknesses.drop(columns=\"zero\", inplace=True)\n",
    "        locations = pd.concat((locations, rolling.iloc[:, -2:]))\n",
    "        df_horizontal = pd.concat((df_horizontal, thicknesses))\n",
    "    horizlogged = df_horizontal.apply(\n",
    "        np.log\n",
    "    )  # take the log of thicknesses for feature engineering\n",
    "    horizpowered = df_horizontal.apply(\n",
    "        lambda x: x ** 10\n",
    "    )  # calculates the power values of thickness for another feature\n",
    "    hs = (\n",
    "        pd.concat(\n",
    "            [df_horizontal, horizlogged, horizpowered, locations],\n",
    "            axis=1,\n",
    "            join_axes=[df_horizontal.index],\n",
    "        )\n",
    "        .dropna()\n",
    "        .replace(-np.inf, 0)\n",
    "    )\n",
    "    print('normalizing the horizontal strata')\n",
    "    # NORMALIZING THE DATA\n",
    "    # normalize the data from 0 to 1\n",
    "    normalized_dfh = (hs - hs.min()) / (hs.max() - hs.min()).replace(0, 0.00001)\n",
    "    normalized_locations = (locations - locations.min()) / (\n",
    "        locations.max() - locations.min()\n",
    "    )\n",
    "    x = normalized_locations.ex.values\n",
    "    y = normalized_locations.ey.values\n",
    "    normalized_dfh[\"ex\"] = x\n",
    "    normalized_dfh[\"ey\"] = y\n",
    "\n",
    "    # now assign classes to the datasets, 1 is onlap, 0 is angular unconformity\n",
    "    normalized_dfa[\"class\"] = 0 #truncation\n",
    "    normalized_dfo[\"class\"] = 1 #onlap\n",
    "    normalized_dfh[\"class\"] = 2 #horizontal\n",
    "\n",
    "    dataset = pd.concat((normalized_dfa, normalized_dfo, normalized_dfh))\n",
    "    dataset.columns = flat_features\n",
    "    print(f'saving the training data for {no_of_neighbors}')\n",
    "    dataset.to_csv(\n",
    "        r\"F:\\\\Geology\\\\WSGS\\\\Projects\\\\jupyter\\\\\"+str(no_of_neighbors)+\"neighbors.csv\"\n",
    "    )\n",
    "    # dataset = pd.read_csv(r'D:\\\\jupyter\\\\'+str(number_of_layers)+'layers20neighbors.csv', index_col=[0])\n",
    "    print(f'Done with {no_of_neighbors} neighbors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34000, 1203)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_dfh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34000, 1203)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_dfo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['one', 'ex', 'ey', 'class'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_dfa.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
