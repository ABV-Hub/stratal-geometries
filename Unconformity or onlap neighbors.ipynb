{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven',\n",
    "         'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen'] #this creates dummy names for the formations\n",
    "number_of_layers = 7 #this is the number of tops you want in your training data\n",
    "smallest = -5\n",
    "largest = 7\n",
    "step = 0.1\n",
    "no_of_neighbors = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame()\n",
    "locations = pd.DataFrame()\n",
    "elevation_random = sorted(np.random.uniform(smallest, largest, number_of_layers-1))\n",
    "for j in np.arange(smallest, largest, step):\n",
    "    rolling = pd.DataFrame()\n",
    "    for i in range(len(names[0:number_of_layers-1])):\n",
    "        basement = 10*np.sin(1-np.arange(0,40,0.1)/15.28)\n",
    "        elevation = np.full(400, j)\n",
    "        topbasement = np.where(basement > elevation, elevation, basement)\n",
    "        rolling['zero'] = topbasement\n",
    "        layer_elevation = 10*np.sin(1-np.arange(0,40,0.1)/15.28)+elevation_random[i]\n",
    "        layer_elevation = np.where(layer_elevation > elevation, elevation, layer_elevation)\n",
    "        rolling[names[i]] = layer_elevation\n",
    "    x = np.arange(0,40,0.1)\n",
    "    y = np.random.randint(0,10,len(x))\n",
    "    if j%0.2 >0.1:\n",
    "        rolling['ex'] = x*np.cos(-j/2) - y*np.sin(-j/2)\n",
    "        rolling['ey'] = y*np.cos(-j/2) - x*np.sin(-j/2)\n",
    "    else:\n",
    "        rolling['ex'] = x*np.cos(j/2) - y*np.sin(j/2)\n",
    "        rolling['ey'] = y*np.cos(j/2) - x*np.sin(j/2)\n",
    "    for k in range(100):\n",
    "        rolling.iloc[np.random.randint(0,399), np.random.randint(0,number_of_layers-1)] = 0\n",
    "    hood = squareform(pdist(rolling.iloc[:, -2:]))\n",
    "    neighbors = [] \n",
    "    for i in enumerate(hood.argsort()[0:,1:no_of_neighbors+1]):\n",
    "        selected = rolling.iloc[hood.argsort()[i[0],1:no_of_neighbors+1], 0:-2].stack().to_frame().T\n",
    "        selected.columns = selected.columns.droplevel()\n",
    "        neighbors.append(selected)\n",
    "    frame = pd.concat(neighbors, sort=False)\n",
    "    frame.index = range(len(frame))\n",
    "    neighborhood = pd.concat([rolling.iloc[:,:-2], frame], axis=1)\n",
    "    thicknesses  = neighborhood.diff(axis=1)\n",
    "    thicknesses[thicknesses < 0] = 0\n",
    "    thicknesses.drop(columns='zero', inplace=True)\n",
    "    locations = pd.concat((locations, rolling.iloc[:,-2:]))\n",
    "    df = pd.concat((df, thicknesses))\n",
    "logged = df.apply(np.log) #take the log of thicknesses for feature engineering\n",
    "powered = df.apply(lambda x: x**10) #calculates the power values of thickness for another feature\n",
    "at = pd.concat([df, logged, powered, locations], axis=1, join_axes=[df.index]\n",
    "                        ).dropna().replace(-np.inf, 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 380)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZING THE DATA\n",
    "#normalize the data from 0 to 1\n",
    "normalized_dfa = (at-at.min())/(at.max()-at.min()).replace(0,0.00001)\n",
    "normalized_locations = (locations-locations.min())/(locations.max()-locations.min())\n",
    "x = normalized_locations.ex.values\n",
    "y = normalized_locations.ey.values\n",
    "normalized_dfa['ex'] = x\n",
    "normalized_dfa['ey'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onlap =  pd.DataFrame()\n",
    "locations = pd.DataFrame()\n",
    "for j in np.arange(smallest, largest, step):\n",
    "    rolling = pd.DataFrame()\n",
    "    for i in range(len(names[0:number_of_layers-1])):\n",
    "        basement = 10*np.sin(1-np.arange(0,40,0.1)/15.28)\n",
    "        elevation = np.full(400, j)\n",
    "        topbasement = np.where(basement > elevation, elevation, basement)\n",
    "        rolling['zero'] = topbasement\n",
    "        strat_elevation = np.full(400, elevation_random[i])\n",
    "        onlap = np.where(strat_elevation > basement, strat_elevation, basement)\n",
    "        layer_elevation = np.where(onlap > elevation, elevation, onlap)\n",
    "        rolling[names[i]] = layer_elevation\n",
    "    x = np.arange(0,40,0.1)\n",
    "    y = np.random.randint(0,10,len(x))\n",
    "    if j%0.2 >0.1:\n",
    "        rolling['ex'] = x*np.cos(-j/2) - y*np.sin(-j/2)\n",
    "        rolling['ey'] = y*np.cos(-j/2) - x*np.sin(-j/2)\n",
    "    else:\n",
    "        rolling['ex'] = x*np.cos(j/2) - y*np.sin(j/2)\n",
    "        rolling['ey'] = y*np.cos(j/2) - x*np.sin(j/2)\n",
    "    for k in range(100):\n",
    "        rolling.iloc[np.random.randint(0,399), np.random.randint(0,number_of_layers-1)] = 0\n",
    "    hood = squareform(pdist(rolling.iloc[:, -2:]))\n",
    "    neighbors = [] \n",
    "    for i in enumerate(hood.argsort()[0:,1:no_of_neighbors+1]):\n",
    "        selected = rolling.iloc[hood.argsort()[i[0],1:no_of_neighbors+1], 0:-2].stack().to_frame().T\n",
    "        selected.columns = selected.columns.droplevel()\n",
    "        neighbors.append(selected)\n",
    "    frame = pd.concat(neighbors, sort=False)\n",
    "    frame.index = range(len(frame))\n",
    "    neighborhood = pd.concat([rolling.iloc[:,:-2], frame], axis=1)\n",
    "    thicknesses  = neighborhood.diff(axis=1)\n",
    "    thicknesses[thicknesses < 0] = 0\n",
    "    thicknesses.drop(columns='zero', inplace=True)\n",
    "    locations = pd.concat((locations, rolling.iloc[:,-2:]))\n",
    "    df_onlap = pd.concat((df_onlap, thicknesses))\n",
    "onlaplogged = df_onlap.apply(np.log) #take the log of thicknesses for feature engineering\n",
    "onlappowered = df_onlap.apply(lambda x: x**10) #calculates the power values of thickness for another feature\n",
    "ot = pd.concat([df_onlap, onlaplogged, onlappowered, locations], axis=1, join_axes=[df_onlap.index]\n",
    "                        ).dropna().replace(-np.inf, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZING THE DATA\n",
    "#normalize the data from 0 to 1\n",
    "normalized_dfo = (ot-ot.min())/(ot.max()-ot.min()).replace(0,0.00001)\n",
    "normalized_locations = (locations-locations.min())/(locations.max()-locations.min())\n",
    "x = normalized_locations.ex.values\n",
    "y = normalized_locations.ey.values\n",
    "normalized_dfo['ex'] = x\n",
    "normalized_dfo['ey'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_horizontal =  pd.DataFrame()\n",
    "locations = pd.DataFrame()\n",
    "for j in np.arange(smallest, largest, step):\n",
    "    rolling = pd.DataFrame()\n",
    "    for i in range(len(names[0:number_of_layers-1])):\n",
    "        basement = np.full(400, 0)-np.random.rand(400)/100\n",
    "        elevation = np.full(400, j)\n",
    "        topbasement = np.where(basement > elevation, elevation, basement)\n",
    "        rolling['zero'] = topbasement\n",
    "        strat_elevation = np.full(400, elevation_random[i])\n",
    "        layer_elevation = np.where(strat_elevation > elevation, elevation, strat_elevation)\n",
    "        rolling[names[i]] = layer_elevation\n",
    "    x = np.arange(0,40,0.1)\n",
    "    y = np.random.randint(0,10,len(x))\n",
    "    if j%0.2 >0.1:\n",
    "        rolling['ex'] = x*np.cos(-j/2) - y*np.sin(-j/2)\n",
    "        rolling['ey'] = y*np.cos(-j/2) - x*np.sin(-j/2)\n",
    "    else:\n",
    "        rolling['ex'] = x*np.cos(j/2) - y*np.sin(j/2)\n",
    "        rolling['ey'] = y*np.cos(j/2) - x*np.sin(j/2)\n",
    "    for k in range(100):\n",
    "        rolling.iloc[np.random.randint(0,399), np.random.randint(0,number_of_layers-1)] = 0\n",
    "    hood = squareform(pdist(rolling.iloc[:, -2:]))\n",
    "    neighbors = [] \n",
    "    for i in enumerate(hood.argsort()[0:,1:no_of_neighbors+1]):\n",
    "        selected = rolling.iloc[hood.argsort()[i[0],1:no_of_neighbors+1], 0:-2].stack().to_frame().T\n",
    "        selected.columns = selected.columns.droplevel()\n",
    "        neighbors.append(selected)\n",
    "    frame = pd.concat(neighbors, sort=False)\n",
    "    frame.index = range(len(frame))\n",
    "    neighborhood = pd.concat([rolling.iloc[:,:-2], frame], axis=1)\n",
    "    thicknesses  = neighborhood.diff(axis=1)\n",
    "    thicknesses[thicknesses < 0] = 0\n",
    "    thicknesses.drop(columns='zero', inplace=True)\n",
    "    locations = pd.concat((locations, rolling.iloc[:,-2:]))\n",
    "    df_horizontal = pd.concat((df_horizontal, thicknesses))\n",
    "horizlogged = df_horizontal.apply(np.log) #take the log of thicknesses for feature engineering\n",
    "horizpowered = df_horizontal.apply(lambda x: x**10) #calculates the power values of thickness for another feature\n",
    "hs = pd.concat([df_horizontal, horizlogged, horizpowered, locations], axis=1, join_axes=[df_horizontal.index]\n",
    "                        ).dropna().replace(-np.inf, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZING THE DATA\n",
    "#normalize the data from 0 to 1\n",
    "normalized_dfh = (hs-hs.min())/(hs.max()-hs.min()).replace(0,0.00001)\n",
    "normalized_locations = (locations-locations.min())/(locations.max()-locations.min())\n",
    "x = normalized_locations.ex.values\n",
    "y = normalized_locations.ey.values\n",
    "normalized_dfh['ex'] = x\n",
    "normalized_dfh['ey'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now assign classes to the datasets, 1 is onlap, 0 is angular unconformity\n",
    "normalized_dfa['class'] = 0\n",
    "normalized_dfo['class'] = 1\n",
    "normalized_dfh['class'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dfa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dfo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dfh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat((normalized_dfa, normalized_dfo, normalized_dfh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#next let's split our toy data into training and test sets, choose how much with test_size of the data becomes the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.iloc[0:, 0:-1].values, dataset.iloc[0:,-1].values, \n",
    "                                                    test_size=0.1, random_state=86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_encoded = to_categorical(y_train) #this converts the target variable to one-hot encoding\n",
    "y_test_encoded = to_categorical(y_test) #same with the test data conversion to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import keras and some layers, we are going to build a network with two dense layers and a dropout layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the network, we initialize with a dense layer and a relu activation\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.1)) #dropout to avoid overfitting\n",
    "model.add(Dense(1000, kernel_initializer='random_uniform', activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, kernel_initializer='random_uniform', activation='softmax')) #and another dense layer with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy']) \n",
    "#builds the model, with categorical crossentropy for our loss function, optimizing using nadam, and using categorical accuracy\n",
    "#as our accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=10, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, verbose=1, validation_split=0.2) #, callbacks=[tbCallBack]) #Fitting the model for 60 epochs and batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=model.evaluate(X_test, y_test_encoded) #tests the model on the test data\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test) #creates predictions on the test data that we can use\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(y_test, np.argmax(preds, axis=1), normalize=True) #let's visualize the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import some real world data from southwestern Wyoming\n",
    "#this is from the Ro`ck Springs Uplift in Wyoming and has been interpreted as an angular unconformity\n",
    "tops_api = pd.read_csv(r'D:\\jupyter\\EarlyWSGS\\ftunion.csv').fillna(0) #this file is available in the unconformity or onlap folder in the repo\n",
    "#tops = tops_api[['Kl', 'Kll', 'Klz', 'Kfh']]\n",
    "tops = tops_api[['Kfh', 'Klz', 'Kll', 'Kl', 'Tfc', 'Tfob', 'Tfu', 'x', 'y']]\n",
    "locations = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood = squareform(pdist(tops.iloc[:, -2:]))\n",
    "neighbors = []\n",
    "for i in enumerate(hood.argsort()[0:,1:no_of_neighbors+1]):\n",
    "    selected = tops.iloc[hood.argsort()[i[0],1:no_of_neighbors+1], 0:-2].stack().to_frame().T\n",
    "    selected.columns = selected.columns.droplevel()\n",
    "    neighbors.append(selected)\n",
    "frame = pd.concat(neighbors, sort=False)\n",
    "frame.index = range(len(frame))\n",
    "neighborhood = pd.concat([tops.iloc[:,:-2], frame], axis=1)\n",
    "thicknesses  = neighborhood.diff(axis=1)*-1\n",
    "thicknesses[thicknesses < 0] = 0\n",
    "thicknesses.drop(columns='Kfh', inplace=True)\n",
    "thicknesses[thicknesses < 0] = 0\n",
    "thicknesses[thicknesses>3000] = 0\n",
    "locations = tops[['x','y']]\n",
    "real_world_log = thicknesses.apply(np.log) #take the log of thicknesses for feature engineering\n",
    "real_world_pow = thicknesses.apply(lambda x: x**10) #calculates the power values of thickness for another feature\n",
    "rw = pd.concat([thicknesses, real_world_log, real_world_pow, locations], axis=1, join_axes=[thicknesses.index]\n",
    "                        ).dropna().replace(-np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_rw=(rw-rw.min())/(rw.max()-rw.min()).replace(0,0.00001) #normalize the data from 0 to 1\n",
    "real_data = normalized_rw.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_preds = model.predict(real_data) #make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(well_preds, axis=1) #convert them to our binary classification\n",
    "plt.hist(predictions, label = ['Angular Unconformity', 'Onlap']) #visualize the predictions as a histogram, we are expecting all the wells to be classified as '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_api['predictions'] = predictions\n",
    "well_locs = pd.read_csv(r'D:\\jupyter\\EarlyWSGS\\well_locations.csv', encoding = \"ISO-8859-1\")\n",
    "merged = pd.merge(tops_api, well_locs, on='API')\n",
    "plt.scatter(merged[merged['predictions']==0].LON, merged[merged['predictions']==0].LAT, label='Angular Unconformity')\n",
    "plt.scatter(merged[merged['predictions']==1].LON, merged[merged['predictions']==1].LAT, label='Onlap')\n",
    "plt.scatter(merged[merged['predictions']==2].LON, merged[merged['predictions']==2].LAT, label='Horizontally Stratified')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
